abbreviation: CRN
domain:
- CV
task:
- Visual Question Answering
paper:
  title: Hierarchical Conditional Relation Networks for Video Question Answering
  authors:
  - Thao Minh Le
  - Vuong Le
  - Svetha Venkatesh
  - Truyen Tran
  organization:
  - Applied Artificial Intelligence Institute, Deakin University, Australia
  - Applied Artificial Intelligence Institute, Deakin University, Australia
  - Applied Artificial Intelligence Institute, Deakin University, Australia
  - Applied Artificial Intelligence Institute, Deakin University, Australia
  citation_number: 21
  doi: null
  download_url: https://openaccess.thecvf.com/content_CVPR_2020/html/Le_Hierarchical_Conditional_Relation_Networks_for_Video_Question_Answering_CVPR_2020_paper.html
  pages: 9972-9981
  press: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
  publish_year: 2020
  volume: CVPR-2020
citation_papers: null
code: https://github.com/thaolmk54/hcrn-videoqa

abstract: "Video question answering (VideoQA) is challenging as it
requires modeling capacity to distill dynamic visual artifacts
and distant relations and to associate them with linguistic
concepts. We introduce a general-purpose reusable neural
unit called Conditional Relation Network (CRN) that serves
as a building block to construct more sophisticated structures for 
representation and reasoning over video. CRN
takes as input an array of tensorial objects and a conditioning feature, 
and computes an array of encoded output
objects. Model building becomes a simple exercise of replication, 
rearrangement and stacking of these reusable units for
diverse modalities and contextual information. This design
thus supports high-order relational and multi-step reasoning.
The resulting architecture for VideoQA is a CRN hierarchy
whose branches represent sub-videos or clips, all sharing the
same question as the contextual condition. Our evaluations
on well-known datasets achieved new SoTA results, demonstrating the 
impact of building a general-purpose reasoning
unit on complex domains such as VideoQA."

keywords: 

performance:
  dataset:
    MSRVTT-QA:
      Accuracy: 35.6
    MSVD-QA:
      Accuracy: 36.1
    TGIF-QA:
      Action: 75.0
      Count: 3.82
      Frame: 55.9
      Trans: 81.4
  baseline:
    "Tgif-qa Toward spatio-temporal reasoning in visual question answering":
      TGIF-QA:
        Action: 62.9
        Count: 4.32
        Frame: 49.5
        Trans: 69.4
    "Motion-appearance co-memory networks for video question answering":
      TGIF-QA:
        Action: 68.2
        Count: 4.10
        Frame: 51.5
        Trans: 74.3
    "Beyond RNNs: Positional Self-Attention with Co-Attention for Video Question Answering":
      TGIF-QA:
        Action: 70.4
        Count: 4.27
        Frame: 55.7
        Trans: 76.9
    "Heterogeneous memory enhanced multimodal attention model for video question answering":
      TGIF-QA:
        Action: 73.9
        Count: 4.02
        Frame: 77.8
        Trans: 53.8
citation_string: "Le T M , Le V , Venkatesh S , et al. Hierarchical Conditional Relation Networks for Video Question Answering[J]. IEEE, 2020."
