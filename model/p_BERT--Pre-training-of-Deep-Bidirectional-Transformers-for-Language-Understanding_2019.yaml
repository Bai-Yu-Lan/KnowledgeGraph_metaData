abbreviation: BERT
citation_papers: null
code: https://github.com/google-research/bert
domain:
  - NLP
keywords: null
paper:
  authors:
  - Jacob Devlin
  - Ming-Wei Chang
  - Kenton Lee
  - Kristina Toutanova
  citation_number: 48848
  doi: null
  download_url: https://www.aclweb.org/anthology/N19-1423/
  organization:
  - Google AI Language
  - Google AI Language
  - Google AI Language
  - Google AI Language
  pages: 4171--4186
  press: ACL
  publish_year: 2019
  title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'
  volume: NAACL-2019
performance:
  dataset:
    GLUE:
      Avg: 82.1
      CoLA: 60.5
      MNLI-m/mm: 86.7/85.9
      MRPC: 89.3
      QNLI: 92.7
      QQP: 72.1
      RTE: 70.1
      SST-2: 94.9
      STS-B: 86.5
    SQuAD v1.1:
      EM: 87.4
      F1: 93.2
    SQuAD v2.0:
      EM: 80.0
      F1: 83.1
    SWAG:
      Dev: 86.6
      Test: 86.3
baseline:
  'Long Short-Term Memory':
    GLUE:
      Avg: 71.0
      CoLA: 36.0
      MNLI-m/mm: 76.4/76.1
      MRPC: 84.9
      QNLI: 79.8
      QQP: 64.8
      RTE: 56.8
      SST-2: 90.4
      STS-B: 73.3
  'Improving language understanding with unsupervised learning':
    GLUE:
      Avg: 75.1
      CoLA: 45.4
      MNLI-m/mm: 82.1/81.4
      MRPC: 82.3
      QNLI: 87.4
      QQP: 70.3
      RTE: 56.0
      SST-2: 91.3
      STS-B: 80.0
  'Deep contextualized word representations.':
    SQuAD v1.1:
      EM:
      F1: 85.8
    SWAG:
      Dev:
      Test: 78.0
task:
- Natural Language Reasoning
abstract: We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).
citation_string: "Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018."
