abbreviation: XLNet
domain:
  - NLP
task:
  - Natural Language Inference
paper: 
  title: "Generalized Autoregressive Pretraining for Language Understanding"
  authors:
  - Zhilin Yang
  - Zihang Dai
  - Yiming Yang
  - Jaime Carbonell
  - Ruslan Salakhutdinov
  - Quoc V. Le
  organization:
  - Carnegie Mellon University, Google AI Brain Team
  - Carnegie Mellon University, Google AI Brain Team
  - Carnegie Mellon University, Google AI Brain Team
  - Carnegie Mellon University, Google AI Brain Team
  - Carnegie Mellon University, Google AI Brain Team
  - Carnegie Mellon University, Google AI Brain Team
  publish_year: 2019
  volume: NEURIPS-2019
  press: Curran Associates, Inc.
  pages: 5753--5763
  download_url: https://papers.nips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html
  doi: 
  citation_number: 1906
code: https://github.com/zihangdai/xlnet

performance:
  dataset:
    GLUE:
      Dev:
        MNLI-m/mm: 90.8/90.8
        QNLI: 94.9
        QQP: 92.3
        RTE: 85.9
        SST-2: 97.0
        MRPC: 90.8
        CoLA: 69.0
        STS-B: 92.5
      Test:
        MNLI-m/mm: 90.9/90.9
        QNLI: 99.0
        QQP: 90.4
        RTE: 88.5
        SST-2: 97.2
        MRPC: 92.9
        CoLA: 70.2
        STS-B: 93.0
        WNLI: 92.5
    "SQuAD v1.1":
      Dev:
        EM: 89.7
        F1: 95.1
    "SQuAD v2.0":
      Dev:
        EM: 87.9
        F1: 90.6
      Test:
        EM: 87.926
        F1: 90.689
  baseline:
    "Bert: Pre-training of deep bidirectional transformers for language understanding":
      GLUE:
        Dev:
          MNLI-m/mm: 86.6/-
          QNLI: 92.3
          QQP: 91.3
          RTE: 70.4
          SST-2: 93.2
          MRPC: 88.0
          CoLA: 60.6
          STS-B: 90.0
        Test:
          MNLI-m/mm: 87.9/87.4
          QNLI: 96.0
          QQP: 89.9
          RTE: 86.3
          SST-2: 96.5
          MRPC: 92.7
          CoLA: 68.4
          STS-B: 91.1
          WNLI: 89.0
      "SQuAD v1.1":
        Dev:
          EM: 84.1
          F1: 90.9
      "SQuAD v2.0":
        Dev:
          EM: 78.98
          F1: 81.77
        Test:
          EM: 80.005
          F1: 83.061

    "Roberta: A robustly optimized bert pretraining approach":
      GLUE:
        Dev:
          MNLI-m/mm: 90.2/90.2
          QNLI: 94.7
          QQP: 92.2
          RTE: 86.6
          SST-2: 96.4
          MRPC: 90.9
          CoLA: 68.0
          STS-B: 92.4
        Test:
          MNLI-m/mm: 90.8/90.2
          QNLI: 98.9
          QQP: 90.2
          RTE: 88.2
          SST-2: 96.7
          MRPC: 92.3
          CoLA: 67.8
          STS-B: 92.2
          WNLI: 89.0   
      "SQuAD v1.1":
        Dev:
          EM: 88.9
          F1: 94.6

      "SQuAD v2.0":
        Dev:
          EM: 86.5
          F1: 89.4
        Test:
          EM: 86.820
          F1: 89.795


key_words:
citation_papers:
abstract: "With the capability of modeling bidirectional contexts, 
denoising autoencoding based pretraining like BERT achieves better 
performance than pretraining approaches based on autoregressive language 
modeling. However, relying on corrupting the input with masks, BERT 
neglects dependency between the masked positions and suffers from 
a pretrain-finetune discrepancy. In light of these pros and cons, 
we propose XLNet, a generalized autoregressive pretraining method 
that (1) enables learning bidirectional contexts by maximizing 
the expected likelihood over all permutations of the factorization 
order and (2) overcomes the limitations of BERT thanks to its 
autoregressive formulation. Furthermore, XLNet integrates ideas from 
Transformer-XL, the state-of-the-art autoregressive model, into pretraining. 
Empirically, under comparable experiment setting, XLNet outperforms BERT 
on 20 tasks, often by a large margin, including question answering, natural 
language inference, sentiment analysis, and document ranking."
citation_string: "Yang Z, Dai Z, Yang Y, et al. Xlnet: Generalized autoregressive pretraining for language understanding[J]. Advances in neural information processing systems, 2019, 32."
