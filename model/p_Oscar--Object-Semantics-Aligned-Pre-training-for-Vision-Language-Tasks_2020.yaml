abbreviation: Oscar
domain:
  - CV
task:
  - Image-Text Retrieval
  - Image Captioning
  - Novel Object Captioning
  - VQA

paper:
  title: "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"
  authors:
  - Xiujun Li
  - Xi Yin
  - Chunyuan Li
  - Pengchuan Zhang
  - Xiaowei Hu
  - Lei Zhang
  - Lijuan Wang
  - Houdong Hu
  - Li Dong
  - Furu Wei
  - Yejin Choi
  - Jianfeng Gao
  organization:
  - Microsoft Corporation, University of Washington
  - Microsoft Corporation
  - Microsoft Corporation
  - Microsoft Corporation
  - Microsoft Corporation
  - Microsoft Corporation
  - Microsoft Corporation
  - Microsoft Corporation
  - Microsoft Corporation
  - Microsoft Corporation 
  - University of Washington
  - Microsoft Corporation

  publish_year: 2020
  volume: ECCV 2020
  press: ECCV
  pages: 121â€“-137
  download_url: https://link.springer.com/content/pdf/10.1007/978-3-030-58577-8.pdf
  doi: https://doi.org/10.1007/978-3-030-58577-8_8
  citation_number: 702
code: https://github.com/microsoft/Oscar
performance:
  dataset:
    Microsoft COCO:
      BLEU-4: 41.7
      CIDEr: 140.0
      METEOR: 30.6
      SPICE: 24.5
  basekine:
    Attention on Attention for Image Captioning:
      Microsoft COCO:
        BLEU-1: 80.2
        BLEU-4: 38.9
        CIDEr-D: 129.8
        METEOR: 29.2
        ROUGE-L: 58.8
        SPICE: 22.4
    Unified visionlanguage pre-training for image captioning and VQA:
      Microsoft COCO:
        BLEU-4: 39.5
        CIDEr: 129.3
        METEOR: 29.3
        SPICE: 23.2
keywords: 
  - Object semantics
  - Vision-and-language
  - Pre-training
citation_papers: null
abstract: "Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing methods simply concatenate image region features and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner, in this paper, we propose a new learning method OSCAR (Object-Semantics Aligned Pre-training), which uses object tags detected in images as anchor points to significantly ease the learning of alignments. Our method is motivated by the observation that the salient objects in an image can be accurately detected, and are often mentioned in the paired text. We pre-train an OSCAR model on the public corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating new state-of-the-arts on six well-established vision-language understanding and generation tasks."

citation_string: "Li X, Yin X, Li C, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks[C]//European Conference on Computer Vision. Springer, Cham, 2020: 121-137."
